# ğŸ“¡ SimplyStock API ê°€ì´ë“œ

## ë‰´ìŠ¤ API í†µí•© ë°©ë²•

SimplyStockëŠ” ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  AIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

## 1ï¸âƒ£ ë‰´ìŠ¤ ìˆ˜ì§‘ íë¦„

```
ë‰´ìŠ¤ ì†ŒìŠ¤ â†’ ë°ì´í„° ìˆ˜ì§‘ â†’ AI ë¶„ì„ â†’ ë°ì´í„°ë² ì´ìŠ¤ â†’ API â†’ Frontend
```

## 2ï¸âƒ£ ì§€ì›í•˜ëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤

### A. NewsAPI (ì¶”ì²œ)

**íŠ¹ì§•:**
- 70,000+ ë‰´ìŠ¤ ì†ŒìŠ¤
- ì „ ì„¸ê³„ ë‰´ìŠ¤ ì»¤ë²„ë¦¬ì§€
- ë¬´ë£Œ: 100 ìš”ì²­/ì¼, ìœ ë£Œ: $449/ì›”

**ì‚¬ìš© ë°©ë²•:**

```python
# backend/data_collectors/news_collector.py
from newsapi import NewsApiClient

newsapi = NewsApiClient(api_key='YOUR_API_KEY')

# ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰
articles = newsapi.get_everything(
    q='stock market',
    language='en',
    sort_by='publishedAt',
    page_size=20
)
```

**ë“±ë¡:** https://newsapi.org/register

---

### B. Finnhub (ê¸ˆìœµ íŠ¹í™”)

**íŠ¹ì§•:**
- ì£¼ì‹, ì•”í˜¸í™”í, ì™¸í™˜ ë‰´ìŠ¤
- ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
- ë¬´ë£Œ tier í¬í•¨

**ì‚¬ìš© ë°©ë²•:**

```python
import finnhub

finnhub_client = finnhub.Client(api_key="YOUR_API_KEY")

# ì¼ë°˜ ê¸ˆìœµ ë‰´ìŠ¤
news = finnhub_client.general_news('general', min_id=0)

# íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤
company_news = finnhub_client.company_news('AAPL', _from="2024-01-01", to="2024-01-31")
```

**ë“±ë¡:** https://finnhub.io/register

---

### C. Alpha Vantage

**íŠ¹ì§•:**
- ê¸ˆìœµ ë°ì´í„° + ë‰´ìŠ¤
- ë¬´ë£Œ: 5 API ìš”ì²­/ë¶„
- ì£¼ì‹, ì™¸í™˜, ì•”í˜¸í™”í

**ì‚¬ìš© ë°©ë²•:**

```python
import requests

url = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=AAPL&apikey=YOUR_API_KEY'
response = requests.get(url)
data = response.json()
```

**ë“±ë¡:** https://www.alphavantage.co/support/#api-key

---

### D. Yahoo Finance RSS

**íŠ¹ì§•:**
- ì™„ì „ ë¬´ë£Œ
- API í‚¤ ë¶ˆí•„ìš”
- í•œê³„: êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ë°ì´í„°

**ì‚¬ìš© ë°©ë²•:**

```python
import feedparser

feed = feedparser.parse('https://finance.yahoo.com/news/rssindex')
for entry in feed.entries:
    print(entry.title, entry.link)
```

---

### E. í•œêµ­ ì¦ê¶Œ ë‰´ìŠ¤ (í¬ë¡¤ë§)

**ì£¼ì˜ì‚¬í•­:**
- robots.txt í™•ì¸ í•„ìˆ˜
- ì´ìš©ì•½ê´€ ì¤€ìˆ˜
- ë²•ì  ì±…ì„ ê³ ë ¤

**ì¶”ì²œ ì†ŒìŠ¤:**
- í•œêµ­ê²½ì œ: https://www.hankyung.com/
- ë§¤ì¼ê²½ì œ: https://www.mk.co.kr/
- ë„¤ì´ë²„ ê¸ˆìœµ: https://finance.naver.com/
- ë‹¤ìŒ ê¸ˆìœµ: https://finance.daum.net/

**ì˜ˆì‹œ (BeautifulSoup):**

```python
import requests
from bs4 import BeautifulSoup

# ì£¼ì˜: ì‹¤ì œ ì‚¬ìš© ì „ í•´ë‹¹ ì‚¬ì´íŠ¸ ì´ìš©ì•½ê´€ í™•ì¸ í•„ìš”
url = "https://www.hankyung.com/economy"
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# ë‰´ìŠ¤ ì œëª© íŒŒì‹± (ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¦„)
titles = soup.select('.news-title')
for title in titles:
    print(title.text)
```

---

## 3ï¸âƒ£ AI ë¶„ì„ (OpenAI GPT-4)

ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•œ í›„ AIë¡œ ìš”ì•½ ë° ê°ì„± ë¶„ì„:

```python
import openai

openai.api_key = "YOUR_OPENAI_KEY"

def analyze_news(article):
    prompt = f"""
ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì œëª©: {article['title']}
ë‚´ìš©: {article['content']}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
1. í•œ ì¤„ ìš”ì•½
2. ê°ì„± (positive/negative/neutral)
3. ê´€ë ¨ ì£¼ì‹ í‹°ì»¤
"""
    
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
    )
    
    return response.choices[0].message.content
```

---

## 4ï¸âƒ£ ë°ì´í„° íë¦„ ìƒì„¸

### ìˆ˜ì§‘ (Collector)

```python
# backend/data_collectors/news_collector.py

class NewsCollector:
    async def collect_and_process(self):
        # 1. ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘
        articles = []
        articles.extend(self.collect_from_newsapi())
        articles.extend(self.collect_from_finnhub())
        articles.extend(self.collect_from_yahoo_rss())
        
        # 2. AI ë¶„ì„
        for article in articles:
            await self.summarize_with_ai(article)
        
        # 3. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        await self.save_to_database(articles)
        
        return articles
```

### ìŠ¤ì¼€ì¤„ë§ (Celery)

```python
# backend/app/tasks.py

from celery import Celery
from celery.schedules import crontab

celery = Celery('tasks', broker='redis://localhost:6379/0')

@celery.task
def collect_news():
    """ë§¤ ì‹œê°„ë§ˆë‹¤ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    collector = NewsCollector()
    asyncio.run(collector.collect_and_process())

# ìŠ¤ì¼€ì¤„ ì„¤ì •
celery.conf.beat_schedule = {
    'collect-news-hourly': {
        'task': 'app.tasks.collect_news',
        'schedule': crontab(minute=0),  # ë§¤ ì‹œê°„ ì •ê°
    },
}
```

### API ì—”ë“œí¬ì¸íŠ¸

```python
# backend/app/api/news.py

@router.get("/")
async def get_news(
    page: int = 1,
    page_size: int = 20,
    sentiment: Optional[str] = None,
):
    """ë‰´ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
    articles = await db.query(News).filter(
        News.sentiment == sentiment if sentiment else True
    ).offset((page - 1) * page_size).limit(page_size).all()
    
    return {"articles": articles, "total": len(articles)}
```

### Frontend ì‚¬ìš©

```typescript
// frontend/lib/api.ts

export async function getNews(params?: {
  page?: number;
  sentiment?: 'positive' | 'negative' | 'neutral';
}) {
  const url = new URL(`${API_URL}/api/news`);
  if (params?.page) url.searchParams.set('page', params.page.toString());
  if (params?.sentiment) url.searchParams.set('sentiment', params.sentiment);
  
  const response = await fetch(url.toString());
  return response.json();
}
```

---

## 5ï¸âƒ£ ë¹„ìš© ìµœì í™”

### ë¬´ë£Œ API ì¡°í•©

```
NewsAPI (100/ì¼) + Finnhub (ë¬´ë£Œ) + Yahoo RSS (ë¬´ì œí•œ) = ì¶©ë¶„í•œ ë‰´ìŠ¤
```

### ìºì‹± ì „ëµ

```python
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def get_news_cached(cache_key: str, expire_seconds: int = 3600):
    """1ì‹œê°„ ë™ì•ˆ ë‰´ìŠ¤ ìºì‹±"""
    cached = redis_client.get(cache_key)
    
    if cached:
        return json.loads(cached)
    
    # ìºì‹œ ì—†ìœ¼ë©´ ìƒˆë¡œ ìˆ˜ì§‘
    articles = collect_news()
    redis_client.setex(cache_key, expire_seconds, json.dumps(articles))
    
    return articles
```

---

## 6ï¸âƒ£ ì¶”ì²œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ë‰´ìŠ¤ ì†ŒìŠ¤ë“¤     â”‚
â”‚  (API/RSS/í¬ë¡¤)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Celery Worker   â”‚ â† ì£¼ê¸°ì  ìˆ˜ì§‘
â”‚ (ë§¤ ì‹œê°„)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI ë¶„ì„ (GPT-4) â”‚
â”‚ - ìš”ì•½           â”‚
â”‚ - ê°ì„± ë¶„ì„      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL      â”‚
â”‚ + Redis Cache   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Next.js Frontendâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7ï¸âƒ£ ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# 1íšŒì„± ìˆ˜ì§‘
cd backend
python data_collectors/news_collector.py

# ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘
python data_collectors/run_all_collectors.py

# Celeryë¡œ ìë™ ìˆ˜ì§‘
celery -A app.tasks worker --loglevel=info
celery -A app.tasks beat --loglevel=info
```

---

**ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!** ğŸš€

