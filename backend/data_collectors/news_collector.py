"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  AIë¡œ ìš”ì•½/ë¶„ì„í•©ë‹ˆë‹¤.
"""

import os
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import openai

load_dotenv()

class NewsCollector:
    def __init__(self):
        self.newsapi_key = os.getenv("NEWSAPI_KEY")
        self.finnhub_key = os.getenv("FINNHUB_API_KEY")
        self.openai_key = os.getenv("OPENAI_API_KEY")
        
        if self.openai_key:
            openai.api_key = self.openai_key
    
    def collect_from_newsapi(self, query: str = "stock market", page_size: int = 20) -> List[Dict]:
        """
        NewsAPIì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
        https://newsapi.org/
        """
        if not self.newsapi_key:
            print("âš ï¸ NewsAPI key not found")
            return []
        
        url = "https://newsapi.org/v2/everything"
        
        params = {
            "q": query,
            "language": "en",
            "sortBy": "publishedAt",
            "pageSize": page_size,
            "apiKey": self.newsapi_key
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            articles = []
            for article in data.get("articles", []):
                articles.append({
                    "title": article.get("title"),
                    "description": article.get("description"),
                    "content": article.get("content"),
                    "source": article.get("source", {}).get("name"),
                    "author": article.get("author"),
                    "url": article.get("url"),
                    "image_url": article.get("urlToImage"),
                    "published_at": article.get("publishedAt"),
                })
            
            print(f"âœ… NewsAPI: {len(articles)} articles collected")
            return articles
            
        except Exception as e:
            print(f"âŒ NewsAPI error: {e}")
            return []
    
    def collect_from_finnhub(self, category: str = "general") -> List[Dict]:
        """
        Finnhubì—ì„œ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘
        https://finnhub.io/
        """
        if not self.finnhub_key:
            print("âš ï¸ Finnhub key not found")
            return []
        
        url = "https://finnhub.io/api/v1/news"
        
        params = {
            "category": category,
            "token": self.finnhub_key
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            articles = []
            for item in data:
                articles.append({
                    "title": item.get("headline"),
                    "description": item.get("summary"),
                    "source": item.get("source"),
                    "url": item.get("url"),
                    "image_url": item.get("image"),
                    "published_at": datetime.fromtimestamp(item.get("datetime")).isoformat(),
                    "category": item.get("category"),
                })
            
            print(f"âœ… Finnhub: {len(articles)} articles collected")
            return articles
            
        except Exception as e:
            print(f"âŒ Finnhub error: {e}")
            return []
    
    def collect_from_yahoo_finance_rss(self) -> List[Dict]:
        """
        Yahoo Finance RSSì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
        """
        url = "https://finance.yahoo.com/news/rssindex"
        
        try:
            response = requests.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')
            
            articles = []
            for item in items[:20]:  # ìµœê·¼ 20ê°œ
                articles.append({
                    "title": item.find('title').text if item.find('title') else None,
                    "description": item.find('description').text if item.find('description') else None,
                    "url": item.find('link').text if item.find('link') else None,
                    "published_at": item.find('pubDate').text if item.find('pubDate') else None,
                    "source": "Yahoo Finance",
                })
            
            print(f"âœ… Yahoo Finance RSS: {len(articles)} articles collected")
            return articles
            
        except Exception as e:
            print(f"âŒ Yahoo Finance RSS error: {e}")
            return []
    
    def scrape_korean_news(self) -> List[Dict]:
        """
        í•œêµ­ ì¦ê¶Œ ë‰´ìŠ¤ í¬ë¡¤ë§ (ì˜ˆì‹œ: í•œêµ­ê²½ì œ)
        ì£¼ì˜: ì‹¤ì œ ì‚¬ìš© ì‹œ robots.txt í™•ì¸ ë° ì´ìš©ì•½ê´€ ì¤€ìˆ˜ í•„ìš”
        """
        # ì‹¤ì œ êµ¬í˜„ ì‹œ ê° ì‚¬ì´íŠ¸ì˜ robots.txtì™€ ì´ìš©ì•½ê´€ì„ í™•ì¸í•˜ì„¸ìš”
        print("âš ï¸ í•œêµ­ ë‰´ìŠ¤ í¬ë¡¤ë§ì€ ê° ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ì„ í™•ì¸ í›„ êµ¬í˜„í•˜ì„¸ìš”")
        return []
    
    async def summarize_with_ai(self, article: Dict) -> Dict:
        """
        OpenAI GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ìš”ì•½ ë° ê°ì„± ë¶„ì„
        """
        if not self.openai_key:
            print("âš ï¸ OpenAI key not found")
            return article
        
        content = article.get("content") or article.get("description", "")
        if not content or len(content) < 50:
            return article
        
        try:
            # GPT-4ë¥¼ ì‚¬ìš©í•œ ìš”ì•½ ë° ê°ì„± ë¶„ì„
            prompt = f"""
ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì œëª©: {article.get('title')}
ë‚´ìš©: {content}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ìš”ì•½ (2-3 ë¬¸ì¥)
2. ê°ì„± (positive/negative/neutral ì¤‘ í•˜ë‚˜)
3. ê°ì„± ì ìˆ˜ (-1.0 ~ 1.0)
4. ê´€ë ¨ ì£¼ì‹ í‹°ì»¤ (ìˆë‹¤ë©´)
5. ì¹´í…Œê³ ë¦¬ (earnings/m&a/policy/tech/market ì¤‘ í•˜ë‚˜)
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
            )
            
            ai_response = response.choices[0].message.content
            
            # AI ì‘ë‹µ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
            article["ai_summary"] = ai_response
            article["sentiment"] = "neutral"  # íŒŒì‹± í›„ ì„¤ì •
            article["sentiment_score"] = 0.0
            article["tickers"] = []
            article["category"] = "market"
            
            print(f"âœ… AI ë¶„ì„ ì™„ë£Œ: {article.get('title')[:50]}...")
            
        except Exception as e:
            print(f"âŒ AI ë¶„ì„ ì—ëŸ¬: {e}")
        
        return article
    
    async def collect_and_process(self):
        """
        ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì²˜ë¦¬
        """
        print("\nğŸ”„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        print("=" * 60)
        
        all_articles = []
        
        # 1. NewsAPI
        articles = self.collect_from_newsapi(query="stock market OR cryptocurrency")
        all_articles.extend(articles)
        
        # 2. Finnhub
        articles = self.collect_from_finnhub(category="general")
        all_articles.extend(articles)
        
        # 3. Yahoo Finance RSS
        articles = self.collect_from_yahoo_finance_rss()
        all_articles.extend(articles)
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š ì´ {len(all_articles)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # AI ë¶„ì„ (ì„ íƒì )
        if self.openai_key and all_articles:
            print("\nğŸ¤– AI ë¶„ì„ ì‹œì‘...")
            # ì²˜ìŒ 5ê°œë§Œ ë¶„ì„ (ë¹„ìš© ì ˆê°)
            for article in all_articles[:5]:
                await self.summarize_with_ai(article)
        
        # TODO: ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (êµ¬í˜„ ì˜ˆì •)")
        
        return all_articles

async def main():
    collector = NewsCollector()
    articles = await collector.collect_and_process()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìƒ˜í”Œ:")
    print("=" * 60)
    for i, article in enumerate(articles[:3], 1):
        print(f"\n{i}. {article.get('title')}")
        print(f"   ì†ŒìŠ¤: {article.get('source')}")
        print(f"   URL: {article.get('url')}")

if __name__ == "__main__":
    asyncio.run(main())

